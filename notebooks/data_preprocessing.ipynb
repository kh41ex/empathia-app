{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9130ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read csv file\n",
    "df = pd.read_csv('data/combined_dataset_with_comments_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0407f61d",
   "metadata": {},
   "source": [
    "**Step 1: Post & comments lenth and comments score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0360e081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original comments: 366684\n",
      "Valuable comments: 23263\n",
      "Filtered out 343421 low-quality comments.\n"
     ]
    }
   ],
   "source": [
    "# Let's assume 'df' is your cleaned DataFrame with both posts and comments\n",
    "# First, separate them if you haven't already\n",
    "comments_df = df[df['type'] == 'comment'].copy()\n",
    "posts_df = df[df['type'] == 'post'].copy()\n",
    "\n",
    "# Define filtering conditions for COMMENTS\n",
    "min_comment_length = 50  # Minimum number of characters to avoid short/\"RIP\" comments\n",
    "max_comment_length = 500  # Maximum number of characters to avoid overly long comments\n",
    "min_comment_score = 10     # Minimum upvote score\n",
    "\n",
    "# Filter for substantive, upvoted comments\n",
    "valuable_comments_filter = (\n",
    "    (comments_df['cleaned_text'].str.len().between(min_comment_length, max_comment_length)) &\n",
    "    (comments_df['score'] >= min_comment_score)\n",
    ")\n",
    "\n",
    "# Apply the filter\n",
    "valuable_comments_df = comments_df[valuable_comments_filter].copy()\n",
    "\n",
    "print(f\"Original comments: {len(comments_df)}\")\n",
    "print(f\"Valuable comments: {len(valuable_comments_df)}\")\n",
    "print(f\"Filtered out {len(comments_df) - len(valuable_comments_df)} low-quality comments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00928516",
   "metadata": {},
   "source": [
    "**Step 2: Empathic tone sentiment analysis with Textblob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0877ab07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\xiaomi\\watermlops\\kindred_tails\\.venv\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\xiaomi\\watermlops\\kindred_tails\\.venv\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\xiaomi\\watermlops\\kindred_tails\\.venv\\lib\\site-packages (from nltk>=3.9->textblob) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\xiaomi\\watermlops\\kindred_tails\\.venv\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\xiaomi\\watermlops\\kindred_tails\\.venv\\lib\\site-packages (from nltk>=3.9->textblob) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\xiaomi\\watermlops\\kindred_tails\\.venv\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\xiaomi\\watermlops\\kindred_tails\\.venv\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install textblob in your environment if you haven't\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4d9d6443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating sentiment for valuable comments...\n",
      "\n",
      "Empathic and valuable comments: 7004\n",
      "\n",
      "Top 5 most upvoted empathic comments:\n",
      "[Score: 1443, Sentiment: 0.30]: sounds like you the vet did all they could. they really do go downhill fast. i experienced something similar in january when i lost my year old boy. i hope you're ok i send hugs...\n",
      "[Score: 1432, Sentiment: 0.45]: please tell me your dad does not have access to this cat. i honestly fear what he might be willing to do while you're away to better your life....\n",
      "[Score: 1280, Sentiment: 1.00]: im not exaggerating when i say that i would die for antonio. what an absolutely perfect gentleman!...\n",
      "[Score: 1273, Sentiment: 0.42]: you already have sweetheart. i've been rescuing dogs for yrs. he can feel your love. and he won't be alone at the end. that's a beautiful legacy for a dog to have a proud, loving owner until the end. ...\n",
      "[Score: 1193, Sentiment: 0.49]: i think from penelopes perspective it was the best way to die. no stress from going to the vet and fear of getting needles. she got to pass peacefully in her safest, happiest place with her favourite ...\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Define a function to get sentiment polarity\n",
    "def get_sentiment(text):\n",
    "    try:\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    except:\n",
    "        return 0  # Neutral if there's an error\n",
    "\n",
    "# Calculate sentiment for each valuable comment\n",
    "# This might take a minute for large datasets\n",
    "print(\"Calculating sentiment for valuable comments...\")\n",
    "valuable_comments_df['sentiment'] = valuable_comments_df['cleaned_text'].apply(get_sentiment)\n",
    "\n",
    "# Now, filter for comments with positive sentiment (empathy, support)\n",
    "empathic_comments_df = valuable_comments_df[valuable_comments_df['sentiment'] > 0.3].copy()\n",
    "\n",
    "# Sort by the most upvoted AND most positive to see the best examples\n",
    "most_empathic_comments = empathic_comments_df.sort_values(['score', 'sentiment'], ascending=[False, False])\n",
    "\n",
    "print(f\"\\nEmpathic and valuable comments: {len(empathic_comments_df)}\")\n",
    "print(\"\\nTop 5 most upvoted empathic comments:\")\n",
    "for _, row in most_empathic_comments.head(5).iterrows():\n",
    "    print(f\"[Score: {row['score']}, Sentiment: {row['sentiment']:.2f}]: {row['cleaned_text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cbaf4e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original posts: 2687\n",
      "Posts that received valuable comments: 1300\n"
     ]
    }
   ],
   "source": [
    "# Get the unique 'post_id's from our filtered, empathic comments\n",
    "# These are the posts that sparked high-quality discussion\n",
    "valuable_post_ids = empathic_comments_df['post_id'].unique()\n",
    "\n",
    "# Filter the original posts DataFrame to only include these posts\n",
    "valuable_posts_df = posts_df[posts_df['post_id'].isin(valuable_post_ids)].copy()\n",
    "\n",
    "print(f\"Original posts: {len(posts_df)}\")\n",
    "print(f\"Posts that received valuable comments: {len(valuable_posts_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "97f51d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3627 high-quality (post -> response) pairs!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>grieving_post</th>\n",
       "      <th>title</th>\n",
       "      <th>supportive_response</th>\n",
       "      <th>response_score</th>\n",
       "      <th>response_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>1bkf0zz</td>\n",
       "      <td>my first post on reddit. please let me know if...</td>\n",
       "      <td>Very old feral was euthanized. Heartbreak is i...</td>\n",
       "      <td>you are wonderful person. you did what you cou...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.564286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3625</th>\n",
       "      <td>1bkf0zz</td>\n",
       "      <td>my first post on reddit. please let me know if...</td>\n",
       "      <td>Very old feral was euthanized. Heartbreak is i...</td>\n",
       "      <td>you were able to give him the gift of an easy ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626</th>\n",
       "      <td>yk704f</td>\n",
       "      <td>i am newer to reddit and wasn't sure where to ...</td>\n",
       "      <td>feeling guilty over euthanasia</td>\n",
       "      <td>thank you! that's a really good idea to change...</td>\n",
       "      <td>88</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id                                      grieving_post  \\\n",
       "3624  1bkf0zz  my first post on reddit. please let me know if...   \n",
       "3625  1bkf0zz  my first post on reddit. please let me know if...   \n",
       "3626   yk704f  i am newer to reddit and wasn't sure where to ...   \n",
       "\n",
       "                                                  title  \\\n",
       "3624  Very old feral was euthanized. Heartbreak is i...   \n",
       "3625  Very old feral was euthanized. Heartbreak is i...   \n",
       "3626                     feeling guilty over euthanasia   \n",
       "\n",
       "                                    supportive_response  response_score  \\\n",
       "3624  you are wonderful person. you did what you cou...              15   \n",
       "3625  you were able to give him the gift of an easy ...              12   \n",
       "3626  thank you! that's a really good idea to change...              88   \n",
       "\n",
       "      response_sentiment  \n",
       "3624            0.564286  \n",
       "3625            0.458730  \n",
       "3626            0.450000  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the valuable posts with the empathic comments on 'post_id'\n",
    "# This creates a DataFrame where each row is a (post + comment) pair\n",
    "empathic_data = pd.merge(\n",
    "    valuable_posts_df[['post_id', 'cleaned_text', 'title']], # Data from the post\n",
    "    empathic_comments_df[['post_id', 'cleaned_text', 'score', 'sentiment']], # Data from the comment\n",
    "    on='post_id',\n",
    "    how='inner',\n",
    "    suffixes=('_post', '_comment') # This clarifies which 'cleaned_text' is which\n",
    ")\n",
    "\n",
    "# Rename columns for absolute clarity\n",
    "empathic_data.rename(columns={\n",
    "    'cleaned_text_post': 'grieving_post',\n",
    "    'cleaned_text_comment': 'supportive_response',\n",
    "    'score': 'response_score',\n",
    "    'sentiment': 'response_sentiment'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Created {len(empathic_data)} high-quality (post -> response) pairs!\")\n",
    "empathic_data.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dfe612",
   "metadata": {},
   "source": [
    "**Step 3: Self-Reflection Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3398eb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Create a function to detect self-referential comments.\n",
    "\n",
    "import re\n",
    "\n",
    "def is_self_reflective(text, threshold=2):\n",
    "    \"\"\"\n",
    "    Checks if a comment is overly focused on the commenter themselves.\n",
    "    Returns True if the number of first-person pronouns exceeds the threshold.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "        \n",
    "    # Define patterns to look for (using regex for word boundaries)\n",
    "    first_person_patterns = r'\\b(I|me|my|mine|we|us|our|ours)\\b'\n",
    "    \n",
    "    # Find all matches\n",
    "    matches = re.findall(first_person_patterns, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Return True if the number of matches exceeds the threshold\n",
    "    return len(matches) > threshold\n",
    "\n",
    "# Test the function\n",
    "test_comment = \"I'm so sorry. I had to put my dog down last year and I know exactly how you feel. It was the hardest thing I ever did.\"\n",
    "print(is_self_reflective(test_comment, threshold=2)) # Output: True (It has 5 first-person references)\n",
    "\n",
    "test_comment2 = \"I'm so sorry you're going through this. It's the hardest thing. Please be kind to yourself right now.\"\n",
    "print(is_self_reflective(test_comment2, threshold=2)) # Output: False (It has only 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b2777f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valuable, empathic comments: 3627\n",
      "Comments focused on the OP (not the commenter): 2748\n",
      "Filtered out 879 self-reflective comments.\n"
     ]
    }
   ],
   "source": [
    "# Apply the filter to your DataFrame of empathic comments\n",
    "# We want to KEEP comments that are NOT self-reflective\n",
    "focus_on_op_df = empathic_data[~empathic_data['supportive_response'].apply(is_self_reflective)]\n",
    "\n",
    "print(f\"Valuable, empathic comments: {len(empathic_data)}\")\n",
    "print(f\"Comments focused on the OP (not the commenter): {len(focus_on_op_df)}\")\n",
    "print(f\"Filtered out {len(empathic_data) - len(focus_on_op_df)} self-reflective comments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a257539f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>grieving_post</th>\n",
       "      <th>title</th>\n",
       "      <th>supportive_response</th>\n",
       "      <th>response_score</th>\n",
       "      <th>response_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>1bkf0zz</td>\n",
       "      <td>my first post on reddit. please let me know if...</td>\n",
       "      <td>Very old feral was euthanized. Heartbreak is i...</td>\n",
       "      <td>you are wonderful person. you did what you cou...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.564286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3625</th>\n",
       "      <td>1bkf0zz</td>\n",
       "      <td>my first post on reddit. please let me know if...</td>\n",
       "      <td>Very old feral was euthanized. Heartbreak is i...</td>\n",
       "      <td>you were able to give him the gift of an easy ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626</th>\n",
       "      <td>yk704f</td>\n",
       "      <td>i am newer to reddit and wasn't sure where to ...</td>\n",
       "      <td>feeling guilty over euthanasia</td>\n",
       "      <td>thank you! that's a really good idea to change...</td>\n",
       "      <td>88</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id                                      grieving_post  \\\n",
       "3624  1bkf0zz  my first post on reddit. please let me know if...   \n",
       "3625  1bkf0zz  my first post on reddit. please let me know if...   \n",
       "3626   yk704f  i am newer to reddit and wasn't sure where to ...   \n",
       "\n",
       "                                                  title  \\\n",
       "3624  Very old feral was euthanized. Heartbreak is i...   \n",
       "3625  Very old feral was euthanized. Heartbreak is i...   \n",
       "3626                     feeling guilty over euthanasia   \n",
       "\n",
       "                                    supportive_response  response_score  \\\n",
       "3624  you are wonderful person. you did what you cou...              15   \n",
       "3625  you were able to give him the gift of an easy ...              12   \n",
       "3626  thank you! that's a really good idea to change...              88   \n",
       "\n",
       "      response_sentiment  \n",
       "3624            0.564286  \n",
       "3625            0.458730  \n",
       "3626            0.450000  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_on_op_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605be7d",
   "metadata": {},
   "source": [
    "**Step 4: Unsolicited advice filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6a705fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define phrases that often precede unsolicited advice\n",
    "advice_phrases = [\n",
    "    'you should', 'you need to', 'just try to', 'have you tried',\n",
    "    'what I would do is', 'the best thing is to', 'why don\\'t you'\n",
    "]\n",
    "\n",
    "# Create a function to detect advice-heavy comments\n",
    "def is_advice_heavy(text):\n",
    "    text = text.lower()\n",
    "    for phrase in advice_phrases:\n",
    "        if phrase in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Apply the filter (we want to KEEP comments that are NOT advice-heavy)\n",
    "non_advice_df = focus_on_op_df[~focus_on_op_df['supportive_response'].apply(is_advice_heavy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3da39114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2720, 6)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_advice_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eaa15c",
   "metadata": {},
   "source": [
    "**Step 5: Leaving comments with empathy patterns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ae6352df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define positive empathy patterns\n",
    "empathy_patterns = {\n",
    "    'validation': [\n",
    "        r'\\b(normal|natural|understandable|okay|valid|makes sense)\\b',\n",
    "        r'\\b(of course you|it\\'s no wonder|no surprise that|anyone would)\\b',\n",
    "        r'\\b(you have every right|you are (not )?alone|(completely|totally) justified)\\b',\n",
    "        r'\\b(feel that way|go through this|react that way|expected)\\b',\n",
    "        r'\\b(part of the process|part of grieving|part of the journey)\\b'\n",
    "    ],\n",
    "    'affirmation': [\n",
    "        r'\\b(right thing|best decision|loving choice|brave|strong|courageous)\\b',\n",
    "        r'\\b(great pet parent|wonderful owner|amazing friend|did everything you could)\\b',\n",
    "        r'\\b(final act of love|selfless act|put them first|gift of peace)\\b',\n",
    "        r'\\b(they know you loved|they felt your love|honored their life)\\b',\n",
    "        r'\\b(supported them|gave them a great life|fought for them)\\b'\n",
    "    ],\n",
    "    'shared_humanity': [\n",
    "        r'\\b(we all|many of us|so many of us|anyone who has|everyone feels)\\b',\n",
    "        r'\\b(I think most|I believe many|often the case|common experience)\\b',\n",
    "        r'\\b(you are not alone|we understand|we\\'ve been there|here for you)\\b',\n",
    "        r'\\b(this community|in this together|know the pain|share your loss)\\b'\n",
    "    ],\n",
    "    'feeling_words': [\n",
    "        r'\\b(pain|heartbroken|loss|grieving|miss|love|sad|anguish|hurt)\\b',\n",
    "        r'\\b(devastat|mourn|heartache|emptiness|lonely|ache|longing|yearning)\\b',\n",
    "        r'\\b(guilt|guilty|regret|what if|if only|should have|could have)\\b',\n",
    "        r'\\b(thankful|grateful|treasure|blessed|lucky|joy|happy|smile|celebrate)\\b',\n",
    "        r'\\b(peace|peaceful|comfort|healing|hope|better|time|patience|kind)\\b'\n",
    "    ],\n",
    "    'permission_granting': [\n",
    "        r'\\b(allow yourself|give yourself permission|it\\'s alright to)\\b',\n",
    "        r'\\b(you can|you deserve to|you need to|be kind to yourself)\\b',\n",
    "        r'\\b((it\\'s|that\\'s) okay to|permissible|acceptable)\\b'\n",
    "    ],\n",
    "    'present_focus': [\n",
    "        r'\\b(right now|in this moment|today|at this time|for now)\\b',\n",
    "        r'\\b(one (day|step) at a time|moment by moment| breathe|just get through)\\b'\n",
    "    ],\n",
    "    'memory_honoring': [\n",
    "        r'\\b(beautiful memory|wonderful times|remember the love|celebrate their life)\\b',\n",
    "        r'\\b(they would (want|thank)|honor them|keep them in your heart)\\b',\n",
    "        r'\\b(tell us about|share a story|what was their|what did they love)\\b',\n",
    "        r'\\b(paw prints|rainbow bridge|waiting for you|see them again)\\b'\n",
    "    ],\n",
    "    'support_offering': [\n",
    "        r'\\b(I\\'m here|here for you|listening|thinking of you|sending love)\\b',\n",
    "        r'\\b(support|lean on me|reach out|if you need to talk|any time)\\b',\n",
    "        r'\\b(wish I could help|wish I had words|my heart (goes out|is with))\\b'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d0e0c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xiaomi\\AppData\\Local\\Temp\\ipykernel_34944\\3664972126.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  non_advice_df['empathy_score'] = non_advice_df['supportive_response'].apply(calculate_empathy_score)\n"
     ]
    }
   ],
   "source": [
    "def calculate_empathy_score(text):\n",
    "    score = 0\n",
    "    text = text.lower()\n",
    "    for pattern_name, pattern_list in empathy_patterns.items():\n",
    "        for regex in pattern_list:\n",
    "            matches = re.findall(regex, text, flags=re.IGNORECASE)\n",
    "            score += len(matches)\n",
    "    return score\n",
    "\n",
    "# Calculate score for each comment\n",
    "non_advice_df['empathy_score'] = non_advice_df['supportive_response'].apply(calculate_empathy_score)\n",
    "\n",
    "# Filter for comments with a high empathy score\n",
    "empathy_pattern_df = non_advice_df[non_advice_df['empathy_score'] >= 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dc76ff35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1709, 7)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empathy_pattern_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5d7dc032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "876"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empathy_pattern_df.post_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f7928ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>grieving_post</th>\n",
       "      <th>title</th>\n",
       "      <th>supportive_response</th>\n",
       "      <th>response_score</th>\n",
       "      <th>response_sentiment</th>\n",
       "      <th>empathy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3560</th>\n",
       "      <td>5uu29n</td>\n",
       "      <td>i am a wreck. my yr old hound mix fell off a c...</td>\n",
       "      <td>[RIP] My fur baby died suddenly in a hiking ac...</td>\n",
       "      <td>i'm so heartbroken for you but please find com...</td>\n",
       "      <td>53</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3561</th>\n",
       "      <td>5uu29n</td>\n",
       "      <td>i am a wreck. my yr old hound mix fell off a c...</td>\n",
       "      <td>[RIP] My fur baby died suddenly in a hiking ac...</td>\n",
       "      <td>we all will have our time. the contents of tha...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>fclqin</td>\n",
       "      <td>my year old dog hadnt been doing well the past...</td>\n",
       "      <td>[help] my dog died and I didn’t get to say goo...</td>\n",
       "      <td>an excellent point. i've always thought if pos...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>fclqin</td>\n",
       "      <td>my year old dog hadnt been doing well the past...</td>\n",
       "      <td>[help] my dog died and I didn’t get to say goo...</td>\n",
       "      <td>yup. dying adults will often wait for their ch...</td>\n",
       "      <td>40</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>zp6py8</td>\n",
       "      <td>last friday, out of nowhere, we had to put our...</td>\n",
       "      <td>My wife and I lost the bestest of peanuts and ...</td>\n",
       "      <td>she was beautiful. she got lucky when she foun...</td>\n",
       "      <td>73</td>\n",
       "      <td>0.454762</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3571</th>\n",
       "      <td>12vmngr</td>\n",
       "      <td>my dog has ibd and regularly has flare ups. i ...</td>\n",
       "      <td>My father said that when my dog dies, I’ll pro...</td>\n",
       "      <td>it's not wrong to love a soul who loves you un...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3572</th>\n",
       "      <td>12vmngr</td>\n",
       "      <td>my dog has ibd and regularly has flare ups. i ...</td>\n",
       "      <td>My father said that when my dog dies, I’ll pro...</td>\n",
       "      <td>its not wrong to love your dog that much. your...</td>\n",
       "      <td>78</td>\n",
       "      <td>0.321131</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3575</th>\n",
       "      <td>dsyw67</td>\n",
       "      <td>warning this is a long one. tldr at bottom. my...</td>\n",
       "      <td>[Help] How to forgive and love my dog after it...</td>\n",
       "      <td>try trading up games. you become the source of...</td>\n",
       "      <td>31</td>\n",
       "      <td>0.393878</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3578</th>\n",
       "      <td>dsyw67</td>\n",
       "      <td>warning this is a long one. tldr at bottom. my...</td>\n",
       "      <td>[Help] How to forgive and love my dog after it...</td>\n",
       "      <td>exactly. food should never have been in the eq...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3581</th>\n",
       "      <td>hd9q8m</td>\n",
       "      <td>reposted because i forgot the title tag, whoop...</td>\n",
       "      <td>[Fluff] One year ago today, we tried to rehome...</td>\n",
       "      <td>honestly unless theyre super old rehoming isnt...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.367424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3583</th>\n",
       "      <td>hd9q8m</td>\n",
       "      <td>reposted because i forgot the title tag, whoop...</td>\n",
       "      <td>[Fluff] One year ago today, we tried to rehome...</td>\n",
       "      <td>yay! thank you! they are all so freaking fluff...</td>\n",
       "      <td>37</td>\n",
       "      <td>0.302083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3585</th>\n",
       "      <td>1bow9l5</td>\n",
       "      <td>my dog is only years old , i have her since da...</td>\n",
       "      <td>I need to put down my dog i am devastated .</td>\n",
       "      <td>thank you so much for your kind advice and eve...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>dy0ouj</td>\n",
       "      <td>the dogs were more than happy to hop on the ba...</td>\n",
       "      <td>[HELP] It's 2:30 am. I just found 3 friendly d...</td>\n",
       "      <td>you could try the nextdoor app, too. it posts ...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3593</th>\n",
       "      <td>dy0ouj</td>\n",
       "      <td>the dogs were more than happy to hop on the ba...</td>\n",
       "      <td>[HELP] It's 2:30 am. I just found 3 friendly d...</td>\n",
       "      <td>there are pit bull rescue groups in a lot of p...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>1ctz83t</td>\n",
       "      <td>final update radiograph she pooped!!! a good b...</td>\n",
       "      <td>Can’t afford 3000$ surgery for my baby. vet re...</td>\n",
       "      <td>good luck, sending you good thoughts. i know n...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>1ctz83t</td>\n",
       "      <td>final update radiograph she pooped!!! a good b...</td>\n",
       "      <td>Can’t afford 3000$ surgery for my baby. vet re...</td>\n",
       "      <td>if you are a minor, why are your parentsguardi...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3601</th>\n",
       "      <td>17rcstx</td>\n",
       "      <td>my cat died and i need to vent, its going to b...</td>\n",
       "      <td>My cat died</td>\n",
       "      <td>he really was an angel, the sweetest. thank yo...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>hsed18</td>\n",
       "      <td>hello all, my sweet . year old clover louise w...</td>\n",
       "      <td>[RIP] Why you should consider home euthanasia</td>\n",
       "      <td>oh my god. you painted that today?! that's so ...</td>\n",
       "      <td>215</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>hsed18</td>\n",
       "      <td>hello all, my sweet . year old clover louise w...</td>\n",
       "      <td>[RIP] Why you should consider home euthanasia</td>\n",
       "      <td>that is beautiful and so generous! i love what...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605</th>\n",
       "      <td>18i9hr8</td>\n",
       "      <td>just as we were going to sleep last night, my ...</td>\n",
       "      <td>My best buddy Benedict left us last night</td>\n",
       "      <td>the pain will get better overtime and turn int...</td>\n",
       "      <td>21</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>18i9hr8</td>\n",
       "      <td>just as we were going to sleep last night, my ...</td>\n",
       "      <td>My best buddy Benedict left us last night</td>\n",
       "      <td>the vet told us that the best we could have ho...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.396429</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3607</th>\n",
       "      <td>1e7cpg0</td>\n",
       "      <td>we said goodbye to our sweet girl elle today. ...</td>\n",
       "      <td>Goodbye angel</td>\n",
       "      <td>you did everything you could for your sweet gi...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>a4x7kt</td>\n",
       "      <td>when i was , i had a big heart and a small bra...</td>\n",
       "      <td>[Discussion] Letter to My Dog</td>\n",
       "      <td>beautiful. i am sorry for your loss... but so ...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.396667</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>a4x7kt</td>\n",
       "      <td>when i was , i had a big heart and a small bra...</td>\n",
       "      <td>[Discussion] Letter to My Dog</td>\n",
       "      <td>beautiful, thank you for sharing. it sounds li...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.481250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3616</th>\n",
       "      <td>16hrpit</td>\n",
       "      <td>hello reddit, i have lurked a long time but ne...</td>\n",
       "      <td>HELP: Dog of missing for months family member ...</td>\n",
       "      <td>definitely contact the one that was recommende...</td>\n",
       "      <td>135</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3617</th>\n",
       "      <td>16hrpit</td>\n",
       "      <td>hello reddit, i have lurked a long time but ne...</td>\n",
       "      <td>HELP: Dog of missing for months family member ...</td>\n",
       "      <td>your doggone likely just needs to not be with ...</td>\n",
       "      <td>28</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>16hrpit</td>\n",
       "      <td>hello reddit, i have lurked a long time but ne...</td>\n",
       "      <td>HELP: Dog of missing for months family member ...</td>\n",
       "      <td>you dont need a referral to see a dog behavior...</td>\n",
       "      <td>36</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>1esq2zn</td>\n",
       "      <td>hello. my cat, pearl, most likely has cancer i...</td>\n",
       "      <td>Should I be in the room when my cat is put down?</td>\n",
       "      <td>before you finalize anything , make it a point...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>1bkf0zz</td>\n",
       "      <td>my first post on reddit. please let me know if...</td>\n",
       "      <td>Very old feral was euthanized. Heartbreak is i...</td>\n",
       "      <td>you are wonderful person. you did what you cou...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.564286</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3625</th>\n",
       "      <td>1bkf0zz</td>\n",
       "      <td>my first post on reddit. please let me know if...</td>\n",
       "      <td>Very old feral was euthanized. Heartbreak is i...</td>\n",
       "      <td>you were able to give him the gift of an easy ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.458730</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id                                      grieving_post  \\\n",
       "3560   5uu29n  i am a wreck. my yr old hound mix fell off a c...   \n",
       "3561   5uu29n  i am a wreck. my yr old hound mix fell off a c...   \n",
       "3565   fclqin  my year old dog hadnt been doing well the past...   \n",
       "3566   fclqin  my year old dog hadnt been doing well the past...   \n",
       "3567   zp6py8  last friday, out of nowhere, we had to put our...   \n",
       "3571  12vmngr  my dog has ibd and regularly has flare ups. i ...   \n",
       "3572  12vmngr  my dog has ibd and regularly has flare ups. i ...   \n",
       "3575   dsyw67  warning this is a long one. tldr at bottom. my...   \n",
       "3578   dsyw67  warning this is a long one. tldr at bottom. my...   \n",
       "3581   hd9q8m  reposted because i forgot the title tag, whoop...   \n",
       "3583   hd9q8m  reposted because i forgot the title tag, whoop...   \n",
       "3585  1bow9l5  my dog is only years old , i have her since da...   \n",
       "3592   dy0ouj  the dogs were more than happy to hop on the ba...   \n",
       "3593   dy0ouj  the dogs were more than happy to hop on the ba...   \n",
       "3598  1ctz83t  final update radiograph she pooped!!! a good b...   \n",
       "3599  1ctz83t  final update radiograph she pooped!!! a good b...   \n",
       "3601  17rcstx  my cat died and i need to vent, its going to b...   \n",
       "3602   hsed18  hello all, my sweet . year old clover louise w...   \n",
       "3604   hsed18  hello all, my sweet . year old clover louise w...   \n",
       "3605  18i9hr8  just as we were going to sleep last night, my ...   \n",
       "3606  18i9hr8  just as we were going to sleep last night, my ...   \n",
       "3607  1e7cpg0  we said goodbye to our sweet girl elle today. ...   \n",
       "3612   a4x7kt  when i was , i had a big heart and a small bra...   \n",
       "3613   a4x7kt  when i was , i had a big heart and a small bra...   \n",
       "3616  16hrpit  hello reddit, i have lurked a long time but ne...   \n",
       "3617  16hrpit  hello reddit, i have lurked a long time but ne...   \n",
       "3618  16hrpit  hello reddit, i have lurked a long time but ne...   \n",
       "3623  1esq2zn  hello. my cat, pearl, most likely has cancer i...   \n",
       "3624  1bkf0zz  my first post on reddit. please let me know if...   \n",
       "3625  1bkf0zz  my first post on reddit. please let me know if...   \n",
       "\n",
       "                                                  title  \\\n",
       "3560  [RIP] My fur baby died suddenly in a hiking ac...   \n",
       "3561  [RIP] My fur baby died suddenly in a hiking ac...   \n",
       "3565  [help] my dog died and I didn’t get to say goo...   \n",
       "3566  [help] my dog died and I didn’t get to say goo...   \n",
       "3567  My wife and I lost the bestest of peanuts and ...   \n",
       "3571  My father said that when my dog dies, I’ll pro...   \n",
       "3572  My father said that when my dog dies, I’ll pro...   \n",
       "3575  [Help] How to forgive and love my dog after it...   \n",
       "3578  [Help] How to forgive and love my dog after it...   \n",
       "3581  [Fluff] One year ago today, we tried to rehome...   \n",
       "3583  [Fluff] One year ago today, we tried to rehome...   \n",
       "3585        I need to put down my dog i am devastated .   \n",
       "3592  [HELP] It's 2:30 am. I just found 3 friendly d...   \n",
       "3593  [HELP] It's 2:30 am. I just found 3 friendly d...   \n",
       "3598  Can’t afford 3000$ surgery for my baby. vet re...   \n",
       "3599  Can’t afford 3000$ surgery for my baby. vet re...   \n",
       "3601                                        My cat died   \n",
       "3602      [RIP] Why you should consider home euthanasia   \n",
       "3604      [RIP] Why you should consider home euthanasia   \n",
       "3605          My best buddy Benedict left us last night   \n",
       "3606          My best buddy Benedict left us last night   \n",
       "3607                                      Goodbye angel   \n",
       "3612                      [Discussion] Letter to My Dog   \n",
       "3613                      [Discussion] Letter to My Dog   \n",
       "3616  HELP: Dog of missing for months family member ...   \n",
       "3617  HELP: Dog of missing for months family member ...   \n",
       "3618  HELP: Dog of missing for months family member ...   \n",
       "3623   Should I be in the room when my cat is put down?   \n",
       "3624  Very old feral was euthanized. Heartbreak is i...   \n",
       "3625  Very old feral was euthanized. Heartbreak is i...   \n",
       "\n",
       "                                    supportive_response  response_score  \\\n",
       "3560  i'm so heartbroken for you but please find com...              53   \n",
       "3561  we all will have our time. the contents of tha...              10   \n",
       "3565  an excellent point. i've always thought if pos...              11   \n",
       "3566  yup. dying adults will often wait for their ch...              40   \n",
       "3567  she was beautiful. she got lucky when she foun...              73   \n",
       "3571  it's not wrong to love a soul who loves you un...              18   \n",
       "3572  its not wrong to love your dog that much. your...              78   \n",
       "3575  try trading up games. you become the source of...              31   \n",
       "3578  exactly. food should never have been in the eq...              17   \n",
       "3581  honestly unless theyre super old rehoming isnt...              20   \n",
       "3583  yay! thank you! they are all so freaking fluff...              37   \n",
       "3585  thank you so much for your kind advice and eve...              10   \n",
       "3592  you could try the nextdoor app, too. it posts ...              11   \n",
       "3593  there are pit bull rescue groups in a lot of p...              42   \n",
       "3598  good luck, sending you good thoughts. i know n...              14   \n",
       "3599  if you are a minor, why are your parentsguardi...              13   \n",
       "3601  he really was an angel, the sweetest. thank yo...              18   \n",
       "3602  oh my god. you painted that today?! that's so ...             215   \n",
       "3604  that is beautiful and so generous! i love what...              26   \n",
       "3605  the pain will get better overtime and turn int...              21   \n",
       "3606  the vet told us that the best we could have ho...              11   \n",
       "3607  you did everything you could for your sweet gi...              15   \n",
       "3612  beautiful. i am sorry for your loss... but so ...              17   \n",
       "3613  beautiful, thank you for sharing. it sounds li...              10   \n",
       "3616  definitely contact the one that was recommende...             135   \n",
       "3617  your doggone likely just needs to not be with ...              28   \n",
       "3618  you dont need a referral to see a dog behavior...              36   \n",
       "3623  before you finalize anything , make it a point...              13   \n",
       "3624  you are wonderful person. you did what you cou...              15   \n",
       "3625  you were able to give him the gift of an easy ...              12   \n",
       "\n",
       "      response_sentiment  empathy_score  \n",
       "3560            0.500000              3  \n",
       "3561            0.450000              3  \n",
       "3565            0.520000              1  \n",
       "3566            0.333333              1  \n",
       "3567            0.454762              5  \n",
       "3571            0.375000              1  \n",
       "3572            0.321131              4  \n",
       "3575            0.393878              2  \n",
       "3578            0.375000              1  \n",
       "3581            0.367424              1  \n",
       "3583            0.302083              1  \n",
       "3585            0.375000              3  \n",
       "3592            0.406250              1  \n",
       "3593            0.500000              1  \n",
       "3598            0.700000              1  \n",
       "3599            0.337500              1  \n",
       "3601            0.400000              1  \n",
       "3602            0.581250              1  \n",
       "3604            0.750000              1  \n",
       "3605            0.750000              3  \n",
       "3606            0.396429              2  \n",
       "3607            0.350000              2  \n",
       "3612            0.396667              4  \n",
       "3613            0.481250              1  \n",
       "3616            0.400000              2  \n",
       "3617            0.322222              1  \n",
       "3618            0.500000              1  \n",
       "3623            0.560000              1  \n",
       "3624            0.564286              2  \n",
       "3625            0.458730              2  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empathy_pattern_df.tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d28caf",
   "metadata": {},
   "source": [
    "**Step 6: Manually assigning label to relevant replies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5a58696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a file for manual review\n",
    "to_review_df = empathy_pattern_df.sort_values(by=['post_id', 'response_score'], ascending=False).copy()\n",
    "to_review_df['is_relevant'] = 0  # Placeholder for your manual rating\n",
    "to_review_df.to_csv('for_manual_review.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bcb84427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\xiaomi\\watermlops\\kindred_tails\\.venv\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\xiaomi\\watermlops\\kindred_tails\\.venv\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'for_manual_review.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall openpyxl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mfor_manual_review.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfor_manual_review.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Xiaomi\\WaterMLOps\\kindred_tails\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Xiaomi\\WaterMLOps\\kindred_tails\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:2436\u001b[39m, in \u001b[36mNDFrame.to_excel\u001b[39m\u001b[34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   2423\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[32m   2425\u001b[39m formatter = ExcelFormatter(\n\u001b[32m   2426\u001b[39m     df,\n\u001b[32m   2427\u001b[39m     na_rep=na_rep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2434\u001b[39m     inf_rep=inf_rep,\n\u001b[32m   2435\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2445\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Xiaomi\\WaterMLOps\\kindred_tails\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\excel.py:943\u001b[39m, in \u001b[36mExcelFormatter.write\u001b[39m\u001b[34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m     need_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     writer = \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     need_save = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Xiaomi\\WaterMLOps\\kindred_tails\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:61\u001b[39m, in \u001b[36mOpenpyxlWriter.__init__\u001b[39m\u001b[34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[32m     59\u001b[39m engine_kwargs = combine_kwargs(engine_kwargs, kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mif_sheet_exists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_sheet_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# ExcelWriter replaced \"a\" by \"r+\" to allow us to first read the excel file from\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# the file and later write to it\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mode:  \u001b[38;5;66;03m# Load from existing workbook\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Xiaomi\\WaterMLOps\\kindred_tails\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1246\u001b[39m, in \u001b[36mExcelWriter.__init__\u001b[39m\u001b[34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28mself\u001b[39m._handles = IOHandles(\n\u001b[32m   1243\u001b[39m     cast(IO[\u001b[38;5;28mbytes\u001b[39m], path), compression={\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[32m   1244\u001b[39m )\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, ExcelWriter):\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m     \u001b[38;5;28mself\u001b[39m._handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[38;5;28mself\u001b[39m._cur_sheet = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Xiaomi\\WaterMLOps\\kindred_tails\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: 'for_manual_review.xlsx'"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl\n",
    "\n",
    "df = pd.read_csv(\"for_manual_review.csv\")\n",
    "df.to_excel(\"for_manual_review.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e26b46ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual ratings value counts:\n",
      "is_relevant\n",
      "1    1027\n",
      "0     682\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few reviewed rows:\n",
      "   Unnamed: 0                                supportive_response  is_relevant\n",
      "0        1142  so much love in those eyes rdogsmirin all the ...            0\n",
      "1        1143  such love and gratitude in her eyes. rest easy...            0\n",
      "2        1147  thank you wombatfucker's wife for being with m...            0\n",
      "3        3567  she was beautiful. she got lucky when she foun...            1\n",
      "4        2041  tell cash to look for my logan and sam and wid...            0\n",
      "\n",
      "Original DataFrame shape: (1709, 8)\n",
      "Merged DataFrame shape: (1709, 10)\n",
      "\n",
      "'is_relevant' column in final DataFrame:\n",
      "is_relevant_y\n",
      "1    1027\n",
      "0     682\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Final Relevant Dataset size: 1027\n",
      "Rejected: 682\n",
      "Not Reviewed: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. Use your in-memory DataFrame (already available as 'to_review_df')\n",
    "original_df = to_review_df\n",
    "\n",
    "# 2. Load the manually reviewed Excel file\n",
    "reviewed_dataset_df = pd.read_excel('for_manual_review.xlsx')\n",
    "\n",
    "# 3. Inspect the manual ratings (optional but good practice)\n",
    "print(\"Manual ratings value counts:\")\n",
    "print(reviewed_dataset_df['is_relevant'].value_counts())\n",
    "print(\"\\nFirst few reviewed rows:\")\n",
    "print(reviewed_dataset_df[['Unnamed: 0', 'supportive_response', 'is_relevant']].head())\n",
    "\n",
    "# 4. Perform the merge on the 'index' column\n",
    "# We do a 'left' join to keep all rows from the original DataFrame.\n",
    "# We only want to bring in the 'is_relevant' column from the manual file.\n",
    "final_df = original_df.merge(\n",
    "    reviewed_dataset_df[['Unnamed: 0', 'is_relevant']], # Select only the key and the rating\n",
    "    left_index=True, right_on='Unnamed: 0', # Merge using the DataFrame index and Unnamed: 0\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 5. Verify the merge was successful\n",
    "print(f\"\\nOriginal DataFrame shape: {original_df.shape}\")\n",
    "print(f\"Merged DataFrame shape: {final_df.shape}\")\n",
    "print(\"\\n'is_relevant' column in final DataFrame:\")\n",
    "print(final_df['is_relevant_y'].value_counts(dropna=False))\n",
    "\n",
    "# 6. Create your final, curated Golden Dataset\n",
    "# Select only the rows that were marked as 1 (Golden)\n",
    "final_relevant_dataset = final_df[final_df['is_relevant_y'] == 1].copy()\n",
    "\n",
    "# Select rows that were rejected (0) or not reviewed (NaN) for other analysis\n",
    "rejected_dataset = final_df[final_df['is_relevant_y'] == 0]\n",
    "not_reviewed_dataset = final_df[final_df['is_relevant_y'].isna()]\n",
    "\n",
    "print(f\"\\n Final Relevant Dataset size: {len(final_relevant_dataset)}\")\n",
    "print(f\"Rejected: {len(rejected_dataset)}\")\n",
    "print(f\"Not Reviewed: {len(not_reviewed_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6bec8156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the main DataFrame with the new manual rating column attached\n",
    "final_df.to_parquet('final_with_relevance_classificator.parquet', index=False)\n",
    "\n",
    "# Save the most important dataset: your golden examples\n",
    "final_relevant_dataset.to_parquet('final_relevant_training_dataset.parquet', index=False)\n",
    "\n",
    "# Optional: Save the rejected examples for later analysis\n",
    "rejected_dataset.to_parquet('rejected_comments.parquet', index=False)\n",
    "\n",
    "print(\"Datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "62853a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_relevant_dataset.post_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c8474fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>grieving_post</th>\n",
       "      <th>title</th>\n",
       "      <th>supportive_response</th>\n",
       "      <th>response_score</th>\n",
       "      <th>response_sentiment</th>\n",
       "      <th>empathy_score</th>\n",
       "      <th>is_relevant_x</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>is_relevant_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>12zoe05</td>\n",
       "      <td>years wasnt nearly long enough to be wtih you ...</td>\n",
       "      <td>The light has gone out of my life. 13yr Danbi ...</td>\n",
       "      <td>i'm so sorry. she's beautiful. in time the lig...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>809</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>12zoe05</td>\n",
       "      <td>years wasnt nearly long enough to be wtih you ...</td>\n",
       "      <td>The light has gone out of my life. 13yr Danbi ...</td>\n",
       "      <td>i am so sorry for your loss, op. she had the k...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>12zoe05</td>\n",
       "      <td>years wasnt nearly long enough to be wtih you ...</td>\n",
       "      <td>The light has gone out of my life. 13yr Danbi ...</td>\n",
       "      <td>i am sorry for loosing a best friend . but how...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>811</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>12zoe05</td>\n",
       "      <td>years wasnt nearly long enough to be wtih you ...</td>\n",
       "      <td>The light has gone out of my life. 13yr Danbi ...</td>\n",
       "      <td>she loved her parent so much you can tell, and...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>812</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>12rezt4</td>\n",
       "      <td>my cat, milo, died today due to kidney failure...</td>\n",
       "      <td>My 3 yr old cat died while I was away for coll...</td>\n",
       "      <td>milo knows he was loved till the end. cats pic...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>12qrw6t</td>\n",
       "      <td>i'm frustrated with my senior dog's condition ...</td>\n",
       "      <td>I'm frustrated with my senior dog's condition ...</td>\n",
       "      <td>natural death isn't always peaceful, and dying...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>12pu2ch</td>\n",
       "      <td>im putting everything i can think of here. its...</td>\n",
       "      <td>Baby Tokyo had to go to sleep last night. I de...</td>\n",
       "      <td>shes beautiful and what wonderful whisker fire...</td>\n",
       "      <td>91</td>\n",
       "      <td>0.312143</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3097</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>12pu2ch</td>\n",
       "      <td>im putting everything i can think of here. its...</td>\n",
       "      <td>Baby Tokyo had to go to sleep last night. I de...</td>\n",
       "      <td>rest easy tokyo. you were loved and the world ...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3098</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>12pu2ch</td>\n",
       "      <td>im putting everything i can think of here. its...</td>\n",
       "      <td>Baby Tokyo had to go to sleep last night. I de...</td>\n",
       "      <td>did she matter to you? that is all that is imp...</td>\n",
       "      <td>23</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3099</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>12pu2ch</td>\n",
       "      <td>im putting everything i can think of here. its...</td>\n",
       "      <td>Baby Tokyo had to go to sleep last night. I de...</td>\n",
       "      <td>toki is an absolute sweetie! i bet they are pl...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>12jivlb</td>\n",
       "      <td>and maybe writing about her will help with the...</td>\n",
       "      <td>My dog passed yesterday. She was a good girl.</td>\n",
       "      <td>being with her is the most important thing. yo...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>12j57e8</td>\n",
       "      <td>winnie will be euthanized tomorrow morning at ...</td>\n",
       "      <td>Winnie's final night. I have a request.</td>\n",
       "      <td>winnie is beautiful and looks very loved. you ...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.429583</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>12j57e8</td>\n",
       "      <td>winnie will be euthanized tomorrow morning at ...</td>\n",
       "      <td>Winnie's final night. I have a request.</td>\n",
       "      <td>tomorrow will be difficult but i'm glad you we...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.314583</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2555</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>12awces</td>\n",
       "      <td>my sweet senior kitty had surgery last october...</td>\n",
       "      <td>Said goodbye to my 17 year old kitty, the grie...</td>\n",
       "      <td>condolences on the loss of your sweet, soulful...</td>\n",
       "      <td>59</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>12awces</td>\n",
       "      <td>my sweet senior kitty had surgery last october...</td>\n",
       "      <td>Said goodbye to my 17 year old kitty, the grie...</td>\n",
       "      <td>thank you for adopting this gorgeous senior gi...</td>\n",
       "      <td>43</td>\n",
       "      <td>0.555612</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1725</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>12at1u8</td>\n",
       "      <td>my younger girl passed away on january th. she...</td>\n",
       "      <td>My second dog passed in 6 weeks</td>\n",
       "      <td>youre killin me with the picture. they look so...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3527</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>12aqt94</td>\n",
       "      <td>i put my dog mate down on his th birthday two ...</td>\n",
       "      <td>I don’t want to move on. I don’t want to accep...</td>\n",
       "      <td>at that age there is nothing you can do. it wa...</td>\n",
       "      <td>86</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2513</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>11z1cgc</td>\n",
       "      <td>i dont want to over use the word trauma but it...</td>\n",
       "      <td>Is it weird to say pet loss is traumatic?</td>\n",
       "      <td>i hope you keep posting about her. she was lov...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.564286</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2794</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>11z1cgc</td>\n",
       "      <td>i dont want to over use the word trauma but it...</td>\n",
       "      <td>Is it weird to say pet loss is traumatic?</td>\n",
       "      <td>no, loss of a beloved pet is an objectively tr...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2795</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>11u8cgk</td>\n",
       "      <td>tldr vet made me regret my crisis decision to ...</td>\n",
       "      <td>ER vet: “I wouldn’t want to do that to my own ...</td>\n",
       "      <td>you don't want him to keep suffering. ice crea...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3416</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id                                      grieving_post  \\\n",
       "1663  12zoe05  years wasnt nearly long enough to be wtih you ...   \n",
       "1664  12zoe05  years wasnt nearly long enough to be wtih you ...   \n",
       "1665  12zoe05  years wasnt nearly long enough to be wtih you ...   \n",
       "1666  12zoe05  years wasnt nearly long enough to be wtih you ...   \n",
       "1677  12rezt4  my cat, milo, died today due to kidney failure...   \n",
       "1679  12qrw6t  i'm frustrated with my senior dog's condition ...   \n",
       "1680  12pu2ch  im putting everything i can think of here. its...   \n",
       "1681  12pu2ch  im putting everything i can think of here. its...   \n",
       "1682  12pu2ch  im putting everything i can think of here. its...   \n",
       "1684  12pu2ch  im putting everything i can think of here. its...   \n",
       "1688  12jivlb  and maybe writing about her will help with the...   \n",
       "1689  12j57e8  winnie will be euthanized tomorrow morning at ...   \n",
       "1690  12j57e8  winnie will be euthanized tomorrow morning at ...   \n",
       "1698  12awces  my sweet senior kitty had surgery last october...   \n",
       "1699  12awces  my sweet senior kitty had surgery last october...   \n",
       "1700  12at1u8  my younger girl passed away on january th. she...   \n",
       "1701  12aqt94  i put my dog mate down on his th birthday two ...   \n",
       "1702  11z1cgc  i dont want to over use the word trauma but it...   \n",
       "1703  11z1cgc  i dont want to over use the word trauma but it...   \n",
       "1704  11u8cgk  tldr vet made me regret my crisis decision to ...   \n",
       "\n",
       "                                                  title  \\\n",
       "1663  The light has gone out of my life. 13yr Danbi ...   \n",
       "1664  The light has gone out of my life. 13yr Danbi ...   \n",
       "1665  The light has gone out of my life. 13yr Danbi ...   \n",
       "1666  The light has gone out of my life. 13yr Danbi ...   \n",
       "1677  My 3 yr old cat died while I was away for coll...   \n",
       "1679  I'm frustrated with my senior dog's condition ...   \n",
       "1680  Baby Tokyo had to go to sleep last night. I de...   \n",
       "1681  Baby Tokyo had to go to sleep last night. I de...   \n",
       "1682  Baby Tokyo had to go to sleep last night. I de...   \n",
       "1684  Baby Tokyo had to go to sleep last night. I de...   \n",
       "1688      My dog passed yesterday. She was a good girl.   \n",
       "1689            Winnie's final night. I have a request.   \n",
       "1690            Winnie's final night. I have a request.   \n",
       "1698  Said goodbye to my 17 year old kitty, the grie...   \n",
       "1699  Said goodbye to my 17 year old kitty, the grie...   \n",
       "1700                    My second dog passed in 6 weeks   \n",
       "1701  I don’t want to move on. I don’t want to accep...   \n",
       "1702          Is it weird to say pet loss is traumatic?   \n",
       "1703          Is it weird to say pet loss is traumatic?   \n",
       "1704  ER vet: “I wouldn’t want to do that to my own ...   \n",
       "\n",
       "                                    supportive_response  response_score  \\\n",
       "1663  i'm so sorry. she's beautiful. in time the lig...              16   \n",
       "1664  i am so sorry for your loss, op. she had the k...              14   \n",
       "1665  i am sorry for loosing a best friend . but how...              13   \n",
       "1666  she loved her parent so much you can tell, and...              10   \n",
       "1677  milo knows he was loved till the end. cats pic...              29   \n",
       "1679  natural death isn't always peaceful, and dying...              20   \n",
       "1680  shes beautiful and what wonderful whisker fire...              91   \n",
       "1681  rest easy tokyo. you were loved and the world ...              33   \n",
       "1682  did she matter to you? that is all that is imp...              23   \n",
       "1684  toki is an absolute sweetie! i bet they are pl...              11   \n",
       "1688  being with her is the most important thing. yo...              10   \n",
       "1689  winnie is beautiful and looks very loved. you ...              15   \n",
       "1690  tomorrow will be difficult but i'm glad you we...              13   \n",
       "1698  condolences on the loss of your sweet, soulful...              59   \n",
       "1699  thank you for adopting this gorgeous senior gi...              43   \n",
       "1700  youre killin me with the picture. they look so...              16   \n",
       "1701  at that age there is nothing you can do. it wa...              86   \n",
       "1702  i hope you keep posting about her. she was lov...              33   \n",
       "1703  no, loss of a beloved pet is an objectively tr...              14   \n",
       "1704  you don't want him to keep suffering. ice crea...              15   \n",
       "\n",
       "      response_sentiment  empathy_score  is_relevant_x  Unnamed: 0  \\\n",
       "1663            0.390000              1              0         809   \n",
       "1664            0.440000              1              0         810   \n",
       "1665            0.426667              2              0         811   \n",
       "1666            0.450000              1              0         812   \n",
       "1677            0.656250              1              0        3173   \n",
       "1679            0.525000              3              0        3210   \n",
       "1680            0.312143              1              0        3097   \n",
       "1681            0.566667              1              0        3098   \n",
       "1682            0.325000              3              0        3099   \n",
       "1684            0.512500              2              0        3104   \n",
       "1688            0.508333              1              0        3195   \n",
       "1689            0.429583              1              0        2554   \n",
       "1690            0.314583              1              0        2555   \n",
       "1698            0.350000              1              0        1724   \n",
       "1699            0.555612              8              0        1725   \n",
       "1700            0.766667              3              0        3527   \n",
       "1701            0.542857              3              0        2513   \n",
       "1702            0.564286              2              0        2794   \n",
       "1703            0.316667              1              0        2795   \n",
       "1704            0.433333              2              0        3416   \n",
       "\n",
       "      is_relevant_y  \n",
       "1663              1  \n",
       "1664              1  \n",
       "1665              1  \n",
       "1666              1  \n",
       "1677              1  \n",
       "1679              1  \n",
       "1680              1  \n",
       "1681              1  \n",
       "1682              1  \n",
       "1684              1  \n",
       "1688              1  \n",
       "1689              1  \n",
       "1690              1  \n",
       "1698              1  \n",
       "1699              1  \n",
       "1700              1  \n",
       "1701              1  \n",
       "1702              1  \n",
       "1703              1  \n",
       "1704              1  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_relevant_dataset.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7713a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
